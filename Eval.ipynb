{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "887f3e5e-5d17-48bb-92d0-55512bbbb84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    GenerationConfig,\n",
    "    LlamaForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    pipeline,\n",
    ")\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from evaluate import evaluator\n",
    "from datasets import Dataset\n",
    "from evaluate import load\n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"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\"\n",
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1daba56d-1098-4ebf-8f96-c0176c0dd168",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "dataset_name = \"atom92/medical_healthwa_all_2.0\"\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 16\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 32\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.05\n",
    "\n",
    "eval_set_size = 0.1\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23922160-207b-4e1d-a39f-06e8a5a14668",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = load_dataset(dataset_name, split=\"train\")\n",
    "splitted_dataset = my_dataset.train_test_split(test_size=eval_set_size, shuffle=True, seed=42)\n",
    "smaller_subset = splitted_dataset[\"train\"].shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f392315c-1e83-45b4-9df3-68890cf55915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"<START_Q> Could you explain what Evans syndrome is? <END_Q><START_A>Evans' Syndrome is an autoimmune disease in which an individual's antibodies attack their own RBCs as well as their platelets. Its overall pathology is therefore effectively a combination of the two autoimmune induced conditions: autoimmune hemolytic anemia and immune thrombocytopenic purpura. Autoimmune hemolytic anemia is a condition in which the red blood cells that normally carry oxygen and carbon dioxide are destroyed by an autoimmune process. Immune thrombocytopenic purpura is a condition in which the platelets in the blood are destroyed by an autoimmune process. Platelets are a component of blood that contribute to the formation of blood clots in the body to prevent bleeding. Patients with Evans syndrome usually appear normal. Physical examination of patients with Evans syndrome is usually remarkable for jaundice, hepatosplenomegaly, and lymphadenopathy. Laboratory findings consistent with the diagnosis of Evans syndrome include anemia, direct coombs test positive, and antineutrophil antibody positive. Pharmacologic medical therapies for Evan's syndrome include prednisone, intravenous immunoglobulin, and rituximab. Surgery is not the first\\xadline treatment option for patients with Evans syndrome. Splenectomy is usually reserved for patients who are unresponsive to treatment. <END_A>\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f82a31f-10a0-4898-8d91-94070e485fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "logging.info(\"Tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffe54af9-9687-41e9-bce4-4a2abf3896d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3f779a97764d27b62499468ab413b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    max_memory={0: \"14GiB\", 1: \"14GiB\"},\n",
    "    offload_folder=\"/tmp/offload\"\n",
    ")\n",
    "model.tie_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac1d42bd-7fa8-401b-813d-9f15382d8c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=254,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    top_p=0.5,\n",
    "    top_k=50,\n",
    "    repetition_penalty=1.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d05b13ad-ae56-4972-9dad-70603b85dbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_evaluator = evaluator(\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e7905c4-4bf2-4eb7-ab32-f30a42e30b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_question_and_context(text):\n",
    "    # Assuming the structure is <START_Q> Question <END_Q><START_A> Answer <END_A>\n",
    "    split_text = text.split(\"<END_Q>\")\n",
    "    question = split_text[0].replace(\"<START_Q>\", \"\").strip()\n",
    "    answer = split_text[1].replace(\"<START_A>\", \"\").split(\"<END_A>\")[0].strip()\n",
    "    return question, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6a8a7ab-f107-44ff-be68-ce9d49ce185c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.07814137883365697, 'precisions': [0.2275635167245476, 0.09164470054557715, 0.05144020230679085, 0.03475454601874263], 'brevity_penalty': 1.0, 'length_ratio': 1.944437862812463, 'translation_length': 16413, 'reference_length': 8441}\n"
     ]
    }
   ],
   "source": [
    "bleu = load(\"bleu\")\n",
    "\n",
    "# Generate predictions and references\n",
    "predictions = []\n",
    "references = []\n",
    "for example in smaller_subset:\n",
    "    question, answer = extract_question_and_context(example[\"text\"])\n",
    "    # print(\"Question\", question)\n",
    "    # print(\"Answer\", answer)\n",
    "    output = pipe(question)[0]['generated_text']\n",
    "    # print(\"Output\", output)\n",
    "    predictions.append(output)\n",
    "    references.append(answer)\n",
    "\n",
    "# print(predictions)\n",
    "# print(references)\n",
    "results = squad_metric.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d312b3d4-b33c-40cd-a5fc-15d873315223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c26378fd6b44445b2e42c507ae9d0c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.2848863345310359, 'rouge2': 0.1295570429903048, 'rougeL': 0.1923236500365043, 'rougeLsum': 0.21727103488653915}\n"
     ]
    }
   ],
   "source": [
    "rouge = load('rouge')\n",
    "results_rouge = rouge.compute(predictions=predictions, references=references)\n",
    "print(results_rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db9ebb8e-a97d-4d98-950b-d7e705eab84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_for_squad(question, context, model_output):\n",
    "#     # Format the data in a way that is compatible with your SQuAD evaluation setup\n",
    "#     formatted_data = {\n",
    "#         \"question\": question,\n",
    "#         \"context\": context,\n",
    "#         \"model_output\": model_output  # The predicted answer\n",
    "#     }\n",
    "#     return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0210ff28-cf08-468b-b412-1d3c5ed75624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smaller_subset = splitted_dataset[\"train\"].shuffle(seed=42).select(range(100))\n",
    "# task_evaluator = evaluator(\"text-generation\")\n",
    "\n",
    "# predictions = []\n",
    "# references = []\n",
    "\n",
    "# # Generate predictions for each example in the dataset\n",
    "# for example in smaller_subset:\n",
    "#     # Extract question and context from 'text'\n",
    "#     question, context = extract_question_and_context(example['text'])\n",
    "\n",
    "#     # Create a prompt for the model\n",
    "#     prompt = example['text']\n",
    "\n",
    "#     # Generate model prediction\n",
    "#     model_output = pipe(prompt)[0]['generated_text']a\n",
    "\n",
    "#     # Append prediction and reference to lists\n",
    "#     predictions.append(model_output)\n",
    "#     references.append({\"answers\": {\"answer_start\": [], \"text\": [context]}})  # Adjust format as needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8fc1b67-c91a-48bb-89ab-bd8e900eb793",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Evaluation module cache file doesn't exist. Please make sure that you call `add` or `add_batch` at least once before calling `compute`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m \u001b[43mtask_evaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_or_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msmaller_subset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msquad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbootstrap\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_resamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm-fine-tune/myenv/lib/python3.10/site-packages/evaluate/evaluator/base.py:261\u001b[0m, in \u001b[0;36mEvaluator.compute\u001b[0;34m(self, model_or_pipeline, data, subset, split, metric, tokenizer, feature_extractor, strategy, confidence_level, n_resamples, device, random_state, input_column, label_column, label_mapping)\u001b[0m\n\u001b[1;32m    258\u001b[0m metric_inputs\u001b[38;5;241m.\u001b[39mupdate(predictions)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# Compute metrics from references and predictions\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m metric_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metric\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfidence_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfidence_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_resamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_resamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# TODO: To clarify why `wer` and `cer` return float\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# even though metric.compute contract says that it\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# returns Optional[dict].\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(metric_results) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n",
      "File \u001b[0;32m~/llm-fine-tune/myenv/lib/python3.10/site-packages/evaluate/evaluator/base.py:527\u001b[0m, in \u001b[0;36mEvaluator.compute_metric\u001b[0;34m(self, metric, metric_inputs, strategy, confidence_level, n_resamples, random_state)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_metric\u001b[39m(\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    519\u001b[0m     metric: EvaluationModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m     random_state: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    525\u001b[0m ):\n\u001b[1;32m    526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute and return metrics.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 527\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmetric_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMETRIC_KWARGS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbootstrap\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    530\u001b[0m         metric_keys \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mkeys()\n",
      "File \u001b[0;32m~/llm-fine-tune/myenv/lib/python3.10/site-packages/evaluate/module.py:451\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_batch(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_finalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilelock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/llm-fine-tune/myenv/lib/python3.10/site-packages/evaluate/module.py:394\u001b[0m, in \u001b[0;36mEvaluationModule._finalize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_buffer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_writer\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;66;03m# Let's acquire a lock on each node files to be sure they are finished writing\u001b[39;00m\n\u001b[0;32m--> 394\u001b[0m     file_paths, filelocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_all_cache_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;66;03m# Read the predictions and references\u001b[39;00m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/llm-fine-tune/myenv/lib/python3.10/site-packages/evaluate/module.py:311\u001b[0m, in \u001b[0;36mEvaluationModule._get_all_cache_files\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_process \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    312\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation module cache file doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist. Please make sure that you call `add` or `add_batch` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mat least once before calling `compute`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m         )\n\u001b[1;32m    315\u001b[0m     file_paths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name]\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Evaluation module cache file doesn't exist. Please make sure that you call `add` or `add_batch` at least once before calling `compute`."
     ]
    }
   ],
   "source": [
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline=pipe,\n",
    "    data=smaller_subset,\n",
    "    metric=\"squad\",\n",
    "    strategy=\"bootstrap\",\n",
    "    n_resamples=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b19bd54-166b-4c05-8ada-caa1f5575d74",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid `input_column` text specified. The dataset contains the following columns: ['predictions', 'references'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m \u001b[43mtask_evaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_or_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msquad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbootstrap\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_resamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm-fine-tune/myenv/lib/python3.10/site-packages/evaluate/evaluator/base.py:245\u001b[0m, in \u001b[0;36mEvaluator.compute\u001b[0;34m(self, model_or_pipeline, data, subset, split, metric, tokenizer, feature_extractor, strategy, confidence_level, n_resamples, device, random_state, input_column, label_column, label_mapping)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# Prepare inputs\u001b[39;00m\n\u001b[1;32m    244\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_data(data\u001b[38;5;241m=\u001b[39mdata, subset\u001b[38;5;241m=\u001b[39msubset, split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[0;32m--> 245\u001b[0m metric_inputs, pipe_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_column\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_column\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_pipeline(\n\u001b[1;32m    247\u001b[0m     model_or_pipeline\u001b[38;5;241m=\u001b[39mmodel_or_pipeline,\n\u001b[1;32m    248\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m    249\u001b[0m     feature_extractor\u001b[38;5;241m=\u001b[39mfeature_extractor,\n\u001b[1;32m    250\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m    251\u001b[0m )\n\u001b[1;32m    252\u001b[0m metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_metric(metric)\n",
      "File \u001b[0;32m~/llm-fine-tune/myenv/lib/python3.10/site-packages/evaluate/evaluator/text_generation.py:67\u001b[0m, in \u001b[0;36mTextGenerationEvaluator.prepare_data\u001b[0;34m(self, data, input_column, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Dataset, input_column: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Dict, DatasetColumn]:\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    Prepare data.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m        `list`:  pipeline inputs.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_required_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_column\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_column\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, DatasetColumn(data, input_column)\n",
      "File \u001b[0;32m~/llm-fine-tune/myenv/lib/python3.10/site-packages/evaluate/evaluator/base.py:317\u001b[0m, in \u001b[0;36mEvaluator.check_required_columns\u001b[0;34m(self, data, columns_names)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_name, column_name \u001b[38;5;129;01min\u001b[39;00m columns_names\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m column_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumn_names:\n\u001b[0;32m--> 317\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m specified. The dataset contains the following columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    319\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid `input_column` text specified. The dataset contains the following columns: ['predictions', 'references']."
     ]
    }
   ],
   "source": [
    "# eval_results = task_evaluator.compute(\n",
    "#     model_or_pipeline=pipe,\n",
    "#     data=eval_dataset,\n",
    "#     metric=\"squad\",\n",
    "#     strategy=\"bootstrap\",\n",
    "#     n_resamples=30\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16609b99-7fe3-4bc8-99b5-1470dd4648c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8431b9a-4631-44e1-95dc-877ff4732f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c55f7ba-c8cb-49d9-af1d-ace6ec789529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
